These datasets are utilized to investigate and evaluate the performance of Large Language Models (LLMs) in fact-checking tasks.

ChechThat! Lab dataset:	used to evaluate the performance of check-worthiness detection. 

|___	 CT22_english_1A_checkworthy_dev.tsv: test data

|___     CT22_english_1A_checkworthy_train.tsv: generate examples for few-shot settings.



AVeriTeC dataset: used to evaluate the performance of English fact verification and explanation generation.

|___	 dev.json: test data

|___	train.json: generate examples for few-shot settings



CHEF dataset: used to evaluate the performance of Chinese fact verification.

|___	test.json: test data

|___	dev.json: generate examples for few-shot settings



X-FACT dataset: used to evaluate the language impact

|___	test.tsv: test data for Arabic, Italian, German, Tamil, and Portuguese.